{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks with PyTorch\n",
        "\n",
        "In this assignment, we are going to train a Neural Networks on the Japanese MNIST dataset. It is composed of 70000 images of handwritten Hiragana characters. The target variables has 10 different classes.\n",
        "\n",
        "Each image is of dimension 28 by 28. But we will flatten them to form a dataset composed of vectors of dimension (784, 1). The training process will be similar as for a structured dataset.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=16TqEl9ESfXYbUpVafXD6h5UpJYGKfMxE' width=\"500\" height=\"200\">\n",
        "\n",
        "Your goal is to run at least 3 experiments and get a model that can achieve 80% accuracy with not much overfitting on this dataset.\n",
        "\n",
        "Some of the code have already been defined for you. You need only to add your code in the sections specified (marked with **TODO**). Some assert statements have been added to verify the expected outputs are correct. If it does throw an error, this means your implementation is behaving as expected.\n",
        "\n",
        "Note: You can only use fully-connected and dropout layers for this assignment. You can not convolution layers for instance"
      ],
      "metadata": {
        "id": "KNyZ-zZxlU6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Required Packages"
      ],
      "metadata": {
        "id": "iOufKqO8mw7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1.1] We are going to use numpy, matplotlib and google.colab packages"
      ],
      "metadata": {
        "id": "b-sGJ26pmz4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTGG80etnMAa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Download Dataset\n",
        "\n",
        "We will store the dataset into your personal Google Drive.\n"
      ],
      "metadata": {
        "id": "Vyky0K3fnEFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.1] Mount Google Drive"
      ],
      "metadata": {
        "id": "ltUMtjG-nX-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "N_FVrXICnMJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.2] Create a folder called `DL_ASG_1` on your Google Drive at the root level"
      ],
      "metadata": {
        "id": "CzLtlKCHnT9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! mkdir -p /content/gdrive/MyDrive/DL/ASG_1"
      ],
      "metadata": {
        "id": "XZicoPks4POW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.3] Navigate to this folder"
      ],
      "metadata": {
        "id": "sToej_3CnePP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/gdrive/MyDrive/DL/ASG_1'"
      ],
      "metadata": {
        "id": "g2oAXToKnpXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.4] Show the list of item on the folder"
      ],
      "metadata": {
        "id": "TnRHIyhlzUwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "Y-xYtezBzQ0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.4] Dowload the dataset files to your Google Drive if required"
      ],
      "metadata": {
        "id": "3vlfobqnnjJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import requests\n",
        "from tqdm import tqdm\n",
        "import os.path\n",
        "\n",
        "def download_file(url):\n",
        "    path = url.split('/')[-1]\n",
        "    if os.path.isfile(path):\n",
        "        print (f\"{path} already exists\")\n",
        "    else:\n",
        "      r = requests.get(url, stream=True)\n",
        "      with open(path, 'wb') as f:\n",
        "          total_length = int(r.headers.get('content-length'))\n",
        "          print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
        "          for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
        "              if chunk:\n",
        "                  f.write(chunk)\n",
        "\n",
        "url_list = [\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'\n",
        "]\n",
        "\n",
        "for url in url_list:\n",
        "    download_file(url)\"\"\""
      ],
      "metadata": {
        "id": "M0owzTC427NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.5] List the content of the folder and confirm files have been dowloaded properly"
      ],
      "metadata": {
        "id": "DVF_Cx7Hny2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "vt6ZKf4fnqkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load Data"
      ],
      "metadata": {
        "id": "fvvfOON36hTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.1] Import the required modules from PyTorch"
      ],
      "metadata": {
        "id": "duFjgsyPoLPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "#import keras packages\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "1zolHKEO7GZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.2] **TODO** Create 2 variables called `img_height` and `img_width` that will both take the value 28"
      ],
      "metadata": {
        "id": "r4Aw5ObQoWdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "img_height = 28\n",
        "img_width = 28"
      ],
      "metadata": {
        "id": "Ip0NFeyjpj79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.3] Create a function that loads a .npz file using numpy and return the content of the `arr_0` key"
      ],
      "metadata": {
        "id": "hmX5SEHkpp63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load(f):\n",
        "    return np.load(f)['arr_0']"
      ],
      "metadata": {
        "id": "5S3cthx57L2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.4] **TODO** Load the 4 files saved on your Google Drive into their respective variables: x_train, y_train, x_test and y_test"
      ],
      "metadata": {
        "id": "8V2Ij9s7qRtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "x_train = load(\"kmnist-train-imgs.npz\")\n",
        "x_test = load(\"kmnist-test-imgs.npz\")\n",
        "y_train =  load(\"kmnist-train-labels.npz\")\n",
        "y_test =  load(\"kmnist-test-labels.npz\")"
      ],
      "metadata": {
        "id": "5XTkRb0lqpEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.5] **TODO** Using matplotlib display the first image from the train set and its target value"
      ],
      "metadata": {
        "id": "3KC12nB7rlbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "plt.imshow(x_train[0], cmap='gray')\n",
        "plt.title(f\"Target Value: {y_train[0]}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AOtWg7bBrwmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Prepare Data"
      ],
      "metadata": {
        "id": "htLk_27ir0B1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.1] **TODO** Reshape the images from the training and testing set to have the channel dimension last. The dimensions should be: (row_number, height, width, channel)"
      ],
      "metadata": {
        "id": "VJEBe30Er33P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "# Reshape the images from the training set\n",
        "x_train = x_train.reshape(x_train.shape[0], img_height, img_width, 1)\n",
        "\n",
        "# Reshape the images from the testing set\n",
        "x_test = x_test.reshape(x_test.shape[0], img_height, img_width, 1)"
      ],
      "metadata": {
        "id": "1yqWleZasxdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.2] **TODO** Cast `x_train` and `x_test` into `float32` decimals"
      ],
      "metadata": {
        "id": "F2f6wvFys2ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "# Cast x_train and x_test into float32 decimals\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "metadata": {
        "id": "FWZmWe73tLXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.3] **TODO** Standardise the images of the training and testing sets. Originally each image contains pixels with value ranging from 0 to 255. after standardisation, the new value range should be from 0 to 1."
      ],
      "metadata": {
        "id": "Z-1Jr0pKs6jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "# Standardize the images of the training and testing sets\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0"
      ],
      "metadata": {
        "id": "RXY1o272t0JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.4] **TODO** Create a variable called `num_classes` that will take the value 10 which corresponds to the number of classes for the target variable"
      ],
      "metadata": {
        "id": "9eH4aZmXt7Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "num_classes = 10"
      ],
      "metadata": {
        "id": "gTnMgLxYuUs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.6] **TODO** Convert the target variable for the training and testing sets to a binary class matrix of dimension (rows, num_classes).\n",
        "\n",
        "For example:\n",
        "- class 0 will become [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "- class 1 will become [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "- class 5 will become [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "- class 9 will become [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
      ],
      "metadata": {
        "id": "iAy0fUJsuyhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "# Convert the target variable for the training & testing set to a binary class matrix\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test = to_categorical(y_test, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "ysNg37Ukwq8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.7] Let's convert the data to PyTorch tensors\n"
      ],
      "metadata": {
        "id": "eyku1HOTb3lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy arrays to pytorch tensors to make torch dataloaders\n",
        "x_train = torch.tensor(x_train.reshape(-1, 784))\n",
        "y_train = torch.tensor(y_train)\n",
        "x_test = torch.tensor(x_test.reshape(-1, 784))\n",
        "y_test = torch.tensor(y_test)"
      ],
      "metadata": {
        "id": "yKBvBIURcmtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the shape of input and labels\n",
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "cAxa5OZRW87W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Define Neural Networks Architecure"
      ],
      "metadata": {
        "id": "0OCorS00wxPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.1] Set the seed in PyTorch for reproducing results\n",
        "\n"
      ],
      "metadata": {
        "id": "7G_L-yqTxI1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "XB8OIC9wrgFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ceate a variable called device that will automatically select a GPU if available. Otherwise it will default to CPU."
      ],
      "metadata": {
        "id": "q7yhlHZ90-5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "b8LWwKp103XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Define the architecture of your Neural Networks and save it into a variable called `model`"
      ],
      "metadata": {
        "id": "5b93U4MixWeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [5.2.1] Model 1: Simple Neural Network"
      ],
      "metadata": {
        "id": "6gVg5bQhTi54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "# Model with one hidden layer and 20% dropout\n",
        "def simple_nn():\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(784, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(64, 10)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "model1 = simple_nn()"
      ],
      "metadata": {
        "id": "gq1f74uKxpkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [5.2.2] Model 2: Deeper Neural Network with L1 Regularisation"
      ],
      "metadata": {
        "id": "cYwElgpYsErR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with 2 hidden layers and 30% dropout with L1 regularisation\n",
        "def model_l1_regularization(reg_strength):\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(784, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(64, 10)\n",
        "    )\n",
        "\n",
        "    # Add L1 regularization\n",
        "    l1_loss = 0\n",
        "    for param in model.parameters():\n",
        "        l1_loss += torch.norm(param, p=1)\n",
        "\n",
        "    # Combine regularization loss with model's loss\n",
        "    model.regularized_loss = reg_strength * l1_loss\n",
        "\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "model2 = model_l1_regularization(reg_strength=0.001)"
      ],
      "metadata": {
        "id": "TEnco4basIip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [5.2.3] Model 3: NN with L2 Regularisation"
      ],
      "metadata": {
        "id": "gnlzxq7JsNOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with 2 hidden layers and 40% dropout with L2 regularisation\n",
        "def model_l2_regularization(reg_strength):\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(784, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(64, 10)\n",
        "    )\n",
        "\n",
        "    # Add L2 regularization\n",
        "    l2_loss = 0\n",
        "    for param in model.parameters():\n",
        "        l2_loss += torch.norm(param, p=2)\n",
        "\n",
        "    # Combine regularization loss with model's loss\n",
        "    model.regularized_loss = reg_strength * l2_loss\n",
        "\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "model3 = model_l2_regularization(reg_strength=0.001)"
      ],
      "metadata": {
        "id": "xXBS6375sSAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move the models to cuda device if available\n",
        "model1.to(device)\n",
        "model2.to(device)\n",
        "model3.to(device)"
      ],
      "metadata": {
        "id": "3EvAeAgX0bqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.3] **TODO** Print the summary of your model"
      ],
      "metadata": {
        "id": "0IvuMQ81xu5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "# Print summary of Model 1\n",
        "summary(model1, (784,))\n",
        "\n",
        "# Print summary of Model 2\n",
        "summary(model2, (784,))\n",
        "\n",
        "# Print summary of Model 3\n",
        "summary(model3, (784,))"
      ],
      "metadata": {
        "id": "gBRm-h5dxvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Train Neural Networks"
      ],
      "metadata": {
        "id": "sOPTnNxtx6MC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.1] **TODO** Create 2 variables called `batch_size` and `epochs` that will  respectively take the values 128 and 500"
      ],
      "metadata": {
        "id": "fsHJzhnAyP4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "batch_size = 128\n",
        "epochs = 500"
      ],
      "metadata": {
        "id": "hNe_Cia0yde-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine x_train and y_train into a TensorDataset\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "# Similarly for x_test and y_test\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "# Define the lengths for train and validation data\n",
        "train_length = int(len(train_dataset) * 0.7)  # 80% for training\n",
        "val_length = len(train_dataset) - train_length  # Remaining 20% for validation\n",
        "\n",
        "# Split the train dataset into train and validation datasets\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_length, val_length])\n",
        "\n",
        "# Create train and validation data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "bKAo4fbCbYmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.2] **TODO** Compile your model with the appropriate loss function, the optimiser of your choice and the accuracy metric"
      ],
      "metadata": {
        "id": "4-bAkzwXyjAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "0WnNAYT6yjci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.3] **TODO** Train your model\n",
        "using the number of epochs defined. Calculate the total loss and save it to a variable called total_loss.\n",
        "Saved train and val losses in two separate variables"
      ],
      "metadata": {
        "id": "iRvM_pEZy7SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "def train_model(model, optimizer, criterion, epochs, train_loader, val_loader, patience):\n",
        "    train_losses = []  # Save train loss over the epochs\n",
        "    val_losses = []    # Save validation loss\n",
        "    best_val_loss = float('inf')   # Save best Validation loss to monitor early stopping\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)  # Move data to the same device as the model\n",
        "            labels = labels.to(device)  # Move target to the same device as the model\n",
        "            optimizer.zero_grad()       # reset the gradients of optmised tensors\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)   # Calculate loss\n",
        "            loss.backward()             # compute gradients for backward propagation\n",
        "            optimizer.step()            # update parameters\n",
        "            train_loss += loss.item() * inputs.size(0)  # calulate batch loss i.e running loss\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()   # set the model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)  # Move data to the same device as the model\n",
        "                labels = labels.to(device)  # Move target to the same device as the model\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "EMzFo2r5JKn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.4] **TODO** Test your model.  Initiate the model.eval() along with torch.no_grad() to turn off the gradients.\n",
        "Evaluating the model on val set.\n"
      ],
      "metadata": {
        "id": "emZ5Ayr88PZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model 1\n",
        "train_loss1, val_loss1  = train_model(model1, optimizer1, criterion, epochs, train_loader, val_loader, 10)\n",
        "\n",
        "# Train Model 2\n",
        "train_loss2, val_loss2 = train_model(model2, optimizer2, criterion, epochs, train_loader, val_loader, 10)\n",
        "\n",
        "# Train Model 3\n",
        "train_loss3, val_loss3 = train_model(model3, optimizer3, criterion, epochs, train_loader, val_loader, 10)"
      ],
      "metadata": {
        "id": "SY9jatm1mLtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.5] Test the model on unseen data i.e test data"
      ],
      "metadata": {
        "id": "hXkRqV6hnUar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "def test_model(model, test_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Turn off gradients for evaluation\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            labels_indices = torch.argmax(labels, dim=1)  # Convert one-hot encoded labels to class indices\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels_indices).sum().item()  # Compare with class indices\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Loss: {test_loss:.4f}, Accuracy: {100 * accuracy:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "bfvBZ3zy8QM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performance on the Test Set:\")\n",
        "print(\"Model 1 results:\")\n",
        "test_model(model1, test_loader, criterion)\n",
        "\n",
        "# Test Model 2\n",
        "print(\"Model 2 results:\")\n",
        "test_model(model2, test_loader, criterion)\n",
        "\n",
        "# Test Model 3\n",
        "print(\"Model 3 results:\")\n",
        "test_model(model3, test_loader, criterion)"
      ],
      "metadata": {
        "id": "VyvnRBevoLx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Analyse Results"
      ],
      "metadata": {
        "id": "vz9uFy_X6oeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.1] **TODO** Display the performance of your model on the training and validatiom sets"
      ],
      "metadata": {
        "id": "ddugPZhZ68Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "print(\"Performance on the Training Set:\")\n",
        "# Test Model 1\n",
        "print(\"Model 1 results:\")\n",
        "test_model(model1, train_loader, criterion)\n",
        "\n",
        "# Test Model 2\n",
        "print(\"Model 2 results:\")\n",
        "test_model(model2, train_loader, criterion)\n",
        "\n",
        "# Test Model 3\n",
        "print(\"Model 3 results:\")\n",
        "test_model(model3, train_loader, criterion)\n",
        "\n",
        "# Display performance on the testing set\n",
        "print(\"Performance on the Validation Set:\")\n",
        "# Test Model 1\n",
        "print(\"Model 1 results:\")\n",
        "test_model(model1, val_loader, criterion)\n",
        "\n",
        "# Test Model 2\n",
        "print(\"Model 2 results:\")\n",
        "test_model(model2, val_loader, criterion)\n",
        "\n",
        "# Test Model 3\n",
        "print(\"Model 3 results:\")\n",
        "test_model(model3, val_loader, criterion)"
      ],
      "metadata": {
        "id": "yihZIPZ_6sql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.2] **TODO** Plot the learning curve of your model"
      ],
      "metadata": {
        "id": "iBTo_xEI7K_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "def plot_learning_curve(train_losses, val_losses):\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# plot graph to see learning over the epochs, train vs val loss\n",
        "plot_learning_curve(train_loss1, val_loss1)\n",
        "plot_learning_curve(train_loss2, val_loss2)\n",
        "plot_learning_curve(train_loss3, val_loss3)"
      ],
      "metadata": {
        "id": "jRt_4W2F7RVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.3] **TODO** Display the confusion matrix on the testing set predictions"
      ],
      "metadata": {
        "id": "qKPu98GR7a17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "# TODO: Define test_model function if not already defined\n",
        "\n",
        "# Calculate confusion matrix\n",
        "def display_confusion_matrix(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    # Get the predictions for the test dataset\n",
        "    predicted_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Turn off gradients for evaluation\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predicted_labels.extend(predicted.tolist())\n",
        "            true_labels.extend(labels.argmax(1).tolist())\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Display confusion matrix on testing set predictions\n",
        "display_confusion_matrix(model1, test_loader)\n",
        "display_confusion_matrix(model2, test_loader)\n",
        "display_confusion_matrix(model3, test_loader)"
      ],
      "metadata": {
        "id": "TkrP9JCgMzpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqF3VqkZp1Dy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}